Amazon(USA) Data Scraping and Cleaning
This repository contains scripts and files for scraping data from the Amazon USA website, cleaning the scraped data, and loading it into a SQLite database.

Requirements
To run the scripts, ensure you have the following requirements installed:

Python 3.x
Scrapy
SQLite3
Installation
Clone this repository to your local machine:
bash
Copy code
git clone  git@github.com:SKC-18/amazon_scrap.git
Install the Python dependencies:
bash
Copy code
pip install -r requirements.txt
Generate an API key for accessing the Amazon web with Proxy Server.
Usage
Scraping Data
Navigate to the spiders folder:
bash
Copy code
cd spiders
Run the Scrapy spider to scrape data:
bash
Copy code
scrapy crawl mtc_spider -o output.csv
To perform a second iteration of scraping, run:
bash
Copy code
scrapy crawl mtc_spider -o output2.csv
Data Cleaning
Run the data cleaning script:
bash
Copy code
python datacleaning.py
This script will clean the scraped data and save it to a new CSV file.

Data Loading
Run the data loading script to load the cleaned data into SQLite:
bash
Copy code
python dataloading.py
This script will create or connect to a SQLite database named product.db and load the cleaned data into a table.

File Structure
spiders/: Contains the Scrapy spider for scraping data.
output.csv: Output CSV file from the first iteration of scraping.
output2.csv: Output CSV file from the second iteration of scraping.
datacleaning.py: Python script for cleaning the scraped data.
dataloading.py: Python script for loading the cleaned data into SQLite.
product.db: SQLite database file containing the loaded data.
License
This project is licensed under the MIT License - see the LICENSE file for details.





